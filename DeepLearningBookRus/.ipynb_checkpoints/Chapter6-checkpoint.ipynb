{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глава 6\n",
    "# Глубокие сети прямого распрастранения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глубокие сети прямого распрастранения, также часто называются нейронными сетями прямого распрастранения или [\"Многослойный перцептрон\"](https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD_%D0%A0%D1%83%D0%BC%D0%B5%D0%BB%D1%8C%D1%85%D0%B0%D1%80%D1%82%D0%B0) (МСП)\n",
    ", являются наиболее существенной частью моделей глубокого обучения. Целью сетей прямого распрастранения является аппроксимировать любую функцию $f^{*}$. \n",
    "Например, для классификатора $y = f^{*}(x)$ транслирующего вход $x$ на $y$ (класс). СПР оределяет трансляцию $y = f(x;\\theta)$ и обучает значение параметра $\\theta$ так что в результате мы получим лучшее приближение функции. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти модели называются \"рапрастраняющиеся напрямую\" потому-то информация проходящая через функцию $f$ и учитывая $x$ в ходе вычислений на выходе получаем $y$. В данном случае нет обратной связи и выход $y$ не подается обратно на вход модели. Нейронные сети прямого рапрастранения могут быть расширены добавлением обратной связи, они называются рекурентными нейронными сетями, они рассмотрены в главе 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "СПР также очень важны для специалистов практикующих машинное обучение. Они являются важной частью во многих применениях в коммерции. Например, сверточные сети используются для распознания объектов на фото, которые в свою очередь являются разновидностью СПР. СПР являются базовой ступенькой на пути к рекурентным сетям, которые часто использутся в приложениях обработки естественного языка. \n",
    "\n",
    "СПР также называют сетями потому что они обычно представлены композицией различных функций. Модель представляет собой связный ориентированный ациклический граф, описывающий как функции взаимодействуют. Например, мы имеем три функции $f^{(1)}$,$f^{(2)}$,$f^{(3)}$ соединенные в цепочку в форме: $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$. Эта структура цепи является наиболее часто используемая в нейронных сетях. В этом случае $f^{(1)}$ - первый слой, $f^{(2)}$ - второй слой и так далее. В конечном итоге длина цепи дает нам *глубину* модели. Из этого термина появилось название *глубинное обучение*. Последний слой в СПР называется *выходным слоем*. Во время обучения мы приближаем $f(x)$ к $f^{*}(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
