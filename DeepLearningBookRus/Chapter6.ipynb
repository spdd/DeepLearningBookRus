{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глава 6\n",
    "# Глубокие сети прямого распрастранения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глубокие сети прямого распрастранения, также часто называются нейронными сетями прямого распрастранения или [\"Многослойный перцептрон\"](https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD_%D0%A0%D1%83%D0%BC%D0%B5%D0%BB%D1%8C%D1%85%D0%B0%D1%80%D1%82%D0%B0) (МСП)\n",
    ", являются наиболее существенной частью моделей глубокого обучения. Целью сетей прямого распрастранения является аппроксимировать любую функцию $f^{*}$. \n",
    "Например, для классификатора $y = f^{*}(x)$ отображающего вход $x$ на $y$ (класс). СПР определяет отображение $y = f(x;\\theta)$ и обучает значение параметра $\\theta$ так что в результате мы получим лучшее приближение функции. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти модели называются \"рапрастраняющиеся напрямую\" потому-то информация проходящая через функцию $f$ и учитывая $x$ в ходе вычислений на выходе получаем $y$. В данном случае нет обратной связи и выход $y$ не подается обратно на вход модели. Нейронные сети прямого рапрастранения могут быть расширены добавлением обратной связи, они называются рекурентными нейронными сетями, они рассмотрены в главе 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "СПР также очень важны для специалистов практикующих машинное обучение. Они являются важной частью во многих применениях в коммерции. Например, сверточные сети используются для распознания объектов на фото, которые в свою очередь являются разновидностью СПР. СПР являются базовой ступенькой на пути к рекурентным сетям, которые часто использутся в приложениях обработки естественного языка. \n",
    "\n",
    "СПР также называют сетями потому что они обычно представлены композицией различных функций. Модель представляет собой связный ориентированный ациклический граф, описывающий как функции взаимодействуют. Например, мы имеем три функции $f^{(1)}$,$f^{(2)}$,$f^{(3)}$ соединенные в цепочку в форме: $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$. Эта структура цепи является наиболее часто используемая в нейронных сетях. В этом случае $f^{(1)}$ - первый слой, $f^{(2)}$ - второй слой и так далее. В конечном итоге длина цепи дает нам *глубину* модели. Из этого термина появилось название *глубинное обучение*. Последний слой в СПР называется *выходным слоем*. Во время обучения мы приближаем $f(x)$ к $f^{*}(x)$. Тренировачные данные обеспечивают нас шумом приближенные примеры $f^{*}(x)$ оцениваемые в различных точках обучения. Каждый пример $x$ соответствует метке $y \\approx f^{*}(x)$. Тренировачные примеры указывают что выходной слой должен делать с каждой точкой $x$; на выходе должно быть значение близкое к $y$. Поведение других слоев не зависит от тренировачных данных. Обучаемый алгоритм должен решать как использовать эти слои для лучшего приближения $f^{*}$. Потому что тренировачные данные не показывают желательный выход для этих слоев, эти слои называются скрытые слои.\n",
    "\n",
    "Наконец эти сети называются нейронными потому что они вдохновлялись нейробиологией, но мало имеют мало общего. Каждый скрытый слой сети как правило вектор. Размерность этих скрытых слоев определяет ширину модели. Каждый элемент вектора может быть интерпретирован как блок  аналогичный нейрону. Можно представить слой как одну функцию вектор->вектору, также можно представить слой множеством блоков которые выполняются параллельно, каждый из которых представляет функцию вектор->скаляр. Каждый блок напоминает нейрон в том смысле, что он получает входные данные от многих других блоков и вычисляет свое значение активации. Идея представления множества слоев в векторном представлении взята из нейробиологии. Выбор функций $f^{(i)}(x)$ для вычисления этих представлений не задан строго ее можо выбрать также свободно, как руководствуются ученые в своих нейрологических наблюдениях о функциях биологических нейров. Однако современные исследования нейронных сетей ориентируется на математические и инженерные дисциплины и цель нейронных сетей не идеально смоделировать человеческий мозг. Лучше думать о СПР как о функциях приближения, которые предназначены для достижения статистического обобщения, иногда полезно сделать некоторые выводы из того, что мы знаем о мозге и вдохновляться этим, а не использовать их в качестве моделей функции мозга.\n",
    "\n",
    "Один из способов понять СПР начать с линейных моделей и рассмотреть как преодолеть их ограничения. Линейные модели, такие как логистическая регрессия или линейная регрессия являются привлекательными, поскольку они эффективно поддаются оптимизации. Линейные модели также имеют очевидный недостаток, модель ограничена линейными функциями, поэтому модель не пригодна для нахождения связи между любыми двумя входными переменными.\n",
    "\n",
    "Для расширения линейных моделей нужно представить нелинейную функцию от х, мы можем применить линейную модель не подавая на вход $x$, а подать трансформированной вход $\\phi(х)$, где $\\phi$ является нелинейной трансформацией. Эквивалентно, мы можем применить *kernel trick*, описанный в п. 5.7.2, чтобы получить нелинейный алгоритм обучения основанный на применении неявного отображения $\\phi$. Мы можем думать о $\\phi$ как набор признаков описывающих $x$, или как новое представление для $x$.\n",
    "\n",
    "Тогда возникает вопрос, как выбрать отображение $\\phi$.\n",
    "\n",
    "1. Одним из вариантов является использование очень общий $\\phi$, такой как непрерывная функция $\\phi$ которая неявно использует ядра машин, основанных *RBF kernel*. Если $\\phi(x)$ достаточно большой размерности, мы всегда можем иметь достаточную пропускную способность, чтобы подогнать обучающий набор, но обобщение на тестовом наборе часто оставляет желать лучшего. Очень общие отображения признаков, как правило, основаны только на принципе локальной гладкости и не кодируют достаточно априорную информацию для решения современных проблем.\n",
    "\n",
    "2. Другой вариант заключается ручная подгонка $\\phi$. До появления глубокого обучения, это был основным подходом. Такой подход требует десятилетий человеческих усилий для каждой отдельной задачи, со специалистами-практиками, специализирующихся в различных областях, таких как распознавание речи или компьютерного зрения с небольшим количеством пересечений между различными областями.\n",
    "\n",
    "3. Стратегия глубинного обучения, чтобы обучить $\\phi$. При таком подходе мы имеем модель $у = f(х, \\theta, w) = \\phi(х; \\theta)^{T}w$. Теперь мы имеем параметр $\\theta$ который мы используем для обучения $\\phi$ из широкого класса функций, а также параметров $w$ которые отображаются из $\\phi(x)$ на желаемый отклик. Это является примером глубокой сети с прогнозированием, с $\\phi$ определяющей скрытый слой. Такой подход является лишь одним из трех, которая решает проблему оптимизации обучения, но выгоды перевешивают вред. При таком подходе мы параметризуем представление в виде $\\phi(x,\\theta)$ и используем алгоритм оптимизации, чтобы найти $\\theta$, который  соответствует хорошему представлению. Если мы хотим, этот подход может использовать преимущество первого подхода, будучи весьма общим мы делаем это с помощью очень широкого семейства $\\phi(x,\\theta)$. Такой подход может также использовать преимущество второго подхода. Практикующие исследователи могут кодировать свои знания, чтобы помочь обобщить путем разработки семейств $\\phi(x,\\theta)$, которые будут хорошо работать. Преимущество заключается в том, что исследователь только должен найти правильное семество функций, а не одну правильную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
